name: Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - performance
          - email
          - calendar

env:
  PYTHON_VERSION: '3.9'
  NODE_VERSION: '18'

jobs:
  # Job 1: Code Quality Checks
  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black isort mypy bandit safety
          if [ -f src/requirements.txt ]; then pip install -r src/requirements.txt; fi

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install Node.js dependencies
        run: |
          npm ci
          npm audit --audit-level=moderate

      - name: Run Python linting (flake8)
        run: |
          flake8 src/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 src/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: Run Python code formatting check (black)
        run: |
          black --check --diff src/ tests/

      - name: Run Python import sorting check (isort)
        run: |
          isort --check-only --diff src/ tests/

      - name: Run security check (bandit)
        run: |
          bandit -r src/ -f json -o bandit-report.json || true
          bandit -r src/ --severity-level medium

      - name: Run dependency security check (safety)
        run: |
          safety check --json --output safety-report.json || true
          safety check

      - name: Run JavaScript linting (ESLint)
        run: |
          npx eslint src/ --ext .js,.jsx --format json --output-file eslint-report.json || true
          npx eslint src/ --ext .js,.jsx

      - name: Upload security reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
            eslint-report.json

  # Job 2: Unit Tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: code-quality
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10', '3.11']
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-xdist pytest-mock pytest-timeout
          if [ -f src/requirements.txt ]; then pip install -r src/requirements.txt; fi

      - name: Create test environment
        run: |
          mkdir -p reports test_results logs
          export USE_MOCK_EMAIL_CLIENT=true
          export USE_MOCK_CALENDAR_CLIENT=true

      - name: Run unit tests
        run: |
          pytest tests/test_suite.py -m "unit and not slow" \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --junit-xml=reports/unit-tests-${{ matrix.python-version }}.xml \
            --tb=short \
            -v

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: unit-test-results-${{ matrix.python-version }}
          path: |
            reports/
            htmlcov/
            coverage.xml

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: matrix.python-version == '3.9'
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  # Job 3: Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event.inputs.test_type == 'integration' || github.event.inputs.test_type == 'all' || github.event_name != 'workflow_dispatch'
    services:
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 3
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-mock pytest-timeout redis celery
          if [ -f src/requirements.txt ]; then pip install -r src/requirements.txt; fi

      - name: Wait for Redis
        run: |
          timeout 30 bash -c 'until redis-cli -h localhost -p 6379 ping; do sleep 1; done'

      - name: Run email integration tests
        env:
          USE_MOCK_EMAIL_CLIENT: true
          REDIS_URL: redis://localhost:6379/0
        run: |
          pytest tests/test_email_flow.py -m "integration" \
            --junit-xml=reports/email-integration-tests.xml \
            --tb=short \
            -v

      - name: Run calendar integration tests
        env:
          USE_MOCK_CALENDAR_CLIENT: true
        run: |
          pytest tests/test_calendar_integration.py -m "integration" \
            --junit-xml=reports/calendar-integration-tests.xml \
            --tb=short \
            -v

      - name: Upload integration test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: reports/

  # Job 4: Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event.inputs.test_type == 'performance' || github.event.inputs.test_type == 'all' || github.event_name == 'schedule'
    timeout-minutes: 30
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-benchmark pytest-timeout psutil
          if [ -f src/requirements.txt ]; then pip install -r src/requirements.txt; fi

      - name: Run performance tests
        run: |
          pytest tests/test_performance.py -m "performance and not slow" \
            --benchmark-json=reports/benchmark-results.json \
            --junit-xml=reports/performance-tests.xml \
            --tb=short \
            -v

      - name: Upload performance results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-test-results
          path: reports/

  # Job 5: End-to-End Tests
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'all'
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-mock fastapi uvicorn
          if [ -f src/requirements.txt ]; then pip install -r src/requirements.txt; fi

      - name: Install Node.js dependencies
        run: npm ci

      - name: Build frontend
        run: npm run build

      - name: Start backend services
        run: |
          # Start FastAPI server in background
          python -m src.main &
          echo $! > fastapi.pid
          
          # Wait for server to start
          timeout 30 bash -c 'until curl -f http://localhost:8000/health; do sleep 1; done'
        env:
          USE_MOCK_EMAIL_CLIENT: true
          USE_MOCK_CALENDAR_CLIENT: true
          REDIS_URL: redis://localhost:6379/0

      - name: Run E2E tests
        run: |
          pytest tests/ -m "not performance and not slow" \
            --junit-xml=reports/e2e-tests.xml \
            --tb=short \
            -v

      - name: Stop services
        if: always()
        run: |
          if [ -f fastapi.pid ]; then
            kill $(cat fastapi.pid) || true
          fi

      - name: Upload E2E test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: e2e-test-results
          path: reports/

  # Job 6: Test Report Generation
  test-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, performance-tests]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test artifacts
        uses: actions/download-artifact@v3

      - name: Generate combined test report
        run: |
          mkdir -p final-report
          
          # Create summary report
          cat > final-report/test-summary.md << 'EOF'
          # Test Suite Summary
          
          Generated on: $(date)
          Commit: ${{ github.sha }}
          Branch: ${{ github.ref_name }}
          
          ## Test Results
          EOF
          
          # Add unit test results
          if [ -d "unit-test-results-3.9" ]; then
            echo "✅ Unit tests completed" >> final-report/test-summary.md
          else
            echo "❌ Unit tests failed" >> final-report/test-summary.md
          fi
          
          # Add integration test results
          if [ -d "integration-test-results" ]; then
            echo "✅ Integration tests completed" >> final-report/test-summary.md
          else
            echo "❌ Integration tests failed" >> final-report/test-summary.md
          fi
          
          # Add performance test results
          if [ -d "performance-test-results" ]; then
            echo "✅ Performance tests completed" >> final-report/test-summary.md
          else
            echo "⚠️ Performance tests skipped or failed" >> final-report/test-summary.md
          fi

      - name: Upload final test report
        uses: actions/upload-artifact@v3
        with:
          name: final-test-report
          path: final-report/

      - name: Comment PR with test results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const path = './final-report/test-summary.md';
            
            if (fs.existsSync(path)) {
              const report = fs.readFileSync(path, 'utf8');
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: report
              });
            }

  # Job 7: Security Scan
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'all'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Run CodeQL Analysis
        uses: github/codeql-action/analyze@v2
        with:
          languages: python, javascript